{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRbD3t1aDqdg"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch import nn,cuda,backends, optim, save, load, no_grad\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder, CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq74VaaHJ0MS"
   },
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "#mean=[0.6870, 0.5849, 0.5098]\n",
    "#std=[0.2588, 0.3198, 0.3642]\n",
    "\n",
    "mean=[0.4914, 0.4822, 0.4465]\n",
    "std=[0.2023, 0.1994, 0.2010]\n",
    "\n",
    "transformations=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomInvert(p=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=mean,std=std),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "data_train = CIFAR10(root='./data', train=True, download=True, transform=transformations)\n",
    "dataloader_train = torch.utils.data.DataLoader(data_train,batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "data_test = CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8OheLDGF5ZW",
    "outputId": "e36a96c1-2cb6-47e8-abbe-699d827f7de1"
   },
   "outputs": [],
   "source": [
    "# Get device for pytorch to run on\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self,debug=\"\"):\n",
    "        super(PrintLayer, self).__init__()\n",
    "        self.debug=debug\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(self.debug,\": \",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdrpiLTbIFz_"
   },
   "outputs": [],
   "source": [
    "# CNN class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,output_features=141,rgb=True, dropout_percent=0.125,conv1_output_layers=36,conv2_output_layers=112):\n",
    "        super().__init__()\n",
    "        if (rgb):\n",
    "            channels=3\n",
    "        else:\n",
    "            channels=1\n",
    "\n",
    "\n",
    "        conv1=nn.Conv2d(channels,conv1_output_layers,3)\n",
    "        relu=nn.ReLU()\n",
    "        pool=nn.MaxPool2d(3,3)\n",
    "        conv2=nn.Conv2d(conv1_output_layers,conv2_output_layers,9)\n",
    "        #input_layer=nn.Linear(33*2*2, 25)\n",
    "        #hl1=nn.Linear(25, 12)\n",
    "        #hl2=nn.Linear(12, 9)\n",
    "        #output_layer=nn.Linear(9, output_features)\n",
    "        flatten=nn.Flatten()\n",
    "        dropout=nn.Dropout(p=dropout_percent)\n",
    "        softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "        output_layer=nn.Linear(conv2_output_layers*2*2, output_features)\n",
    "        #conv2=nn.Conv2d(36,output_features,9)\n",
    "\n",
    "        \n",
    "\n",
    "        self.filtering_relu_stack = nn.Sequential(\n",
    "            conv1,\n",
    "            relu,\n",
    "            #dropout,\n",
    "            pool,\n",
    "            conv2,\n",
    "            #dropout,\n",
    "            #PrintLayer(\"After conv2\"),\n",
    "            relu,\n",
    "            dropout,\n",
    "            flatten,\n",
    "        )\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #input_layer,\n",
    "            #relu,\n",
    "            #dropout,\n",
    "            #hl1,\n",
    "            #relu,\n",
    "            #dropout,\n",
    "            #hl2,\n",
    "            #relu,\n",
    "            #dropout,\n",
    "            output_layer,\n",
    "            softmax,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.filtering_relu_stack(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x=x.view(-1, 33*2*2)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x=self.linear_relu_stack(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model,optimizer=None,filename=\"checkpoint.pt\",params={}):\n",
    "    params[\"model_state_dict\"]=model.state_dict()\n",
    "    if (optimizer is not None):\n",
    "        params[\"optimizer_state_dict\"]=optimizer.state_dict()\n",
    "    save(params, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fw1zk1HISR7"
   },
   "outputs": [],
   "source": [
    "# Funtion to train model\n",
    "def train_model(model, dataloader, checkpoint_data=None,criterion=None,optimizer=None, continue_from_epoch=0, epoches=5, limit_batches=-1,dataloader_test=None,path=\"\"):\n",
    "    if (criterion is None):\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "    if (optimizer is None):\n",
    "        optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "    if (checkpoint_data is None):\n",
    "        time_model=0\n",
    "        best_epoch=(-1,999999999)\n",
    "    else:\n",
    "        time_model=checkpoint_data[\"time\"]\n",
    "        best_epoch=checkpoint_data[\"best_epoch\"]\n",
    "    f=open(path+\"/model_stats.txt\",\"w\")\n",
    "\n",
    "    for epoch in range(continue_from_epoch,continue_from_epoch+epoches):\n",
    "        subloss=0\n",
    "        total_loss=0\n",
    "        start_time_epoch=time.time()\n",
    "        start_time = time.time()\n",
    "        for batch_number,(x_train,y_train) in enumerate(dataloader):\n",
    "            x_train,y_train=x_train.to(device),y_train.to(device)\n",
    "            y_pred=model(x_train)\n",
    "\n",
    "            loss=criterion(y_pred,y_train)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #subloss+=loss.item()\n",
    "            total_loss+=loss.item()\n",
    "            #if (batch_number%100==0):\n",
    "            #    print(f\"Epoch: {epoch}, Batch Number: {batch_number}, Mini loss average: {subloss/100}, Mini training time: {time.time()-start_time} seconds\")#, Model: {model}\")\n",
    "            #    subloss=0\n",
    "            #    start_time = time.time()\n",
    "            #if (limit_batches==batch_number):\n",
    "            #    break\n",
    "        stop_time=time.time()\n",
    "        time_model=time_model+(stop_time-start_time_epoch)\n",
    "        if (total_loss<best_epoch[1]):\n",
    "            best_epoch=(epoch,total_loss)\n",
    "        Accuracy_train=None\n",
    "        Accuracy_test=None\n",
    "        if (dataloader_test is not None):\n",
    "            print (\"Testing...\")\n",
    "            Accuracy_train=test_model(model,dataloader)\n",
    "            Accuracy_test=test_model(model,dataloader_test)\n",
    "        save_checkpoint(model,optimizer,filename=path+\"/checkpoint2.pt\",params={\n",
    "            'epoch': epoch,\n",
    "            'total_loss': total_loss,\n",
    "            'time': time_model,\n",
    "            'best_epoch': best_epoch,\n",
    "            'Accuracy_test': Accuracy_test,\n",
    "            'Accuracy_train': Accuracy_train,\n",
    "            })\n",
    "        print(f\"Epoch {epoch} Loss: {total_loss}, Time: {stop_time-start_time_epoch} seconds, Accuracy on Test: {Accuracy_test}, Accuracy on Train: {Accuracy_train}\")\n",
    "        print(f\"Epoch {epoch} Loss: {total_loss}, Time: {stop_time-start_time_epoch} seconds, Accuracy on Test: {Accuracy_test}, Accuracy on Train: {Accuracy_train}\", file=f)\n",
    "        if (abs(Accuracy_test-Accuracy_train)>0.12):\n",
    "            print(\"Model has oveefitted. Stopping training...\")\n",
    "            print(\"Model has oveefitted. Stopping training...\", file=f)\n",
    "            break\n",
    "    f.close()\n",
    "    return best_epoch\n",
    "\n",
    "# Function to test model\n",
    "def test_model(model,dataloader,limit_batches=-1):\n",
    "    samples_tested=0\n",
    "    correct=0\n",
    "    with no_grad():\n",
    "        for batch_number,(x_test,y_test) in enumerate(dataloader):\n",
    "            x_test,y_test=x_test.to(device),y_test.to(device)\n",
    "            y_pred=model(x_test)\n",
    "\n",
    "\n",
    "            samples_tested+=y_test.size(0)\n",
    "            prediction=torch.max(y_pred,1)[1]\n",
    "            correct+=(prediction==y_test).sum()\n",
    "            #if (batch_number%100==0):\n",
    "            #    print(f\"Batch number: {batch_number}, Accuraccy: {correct/samples_tested}\")\n",
    "            #if (limit_batches==batch_number):\n",
    "            #    break\n",
    "    accuracy= correct/samples_tested\n",
    "    return (accuracy)\n",
    "\n",
    "def test_model_and_classes(model,dataloader,limit_batches=-1):\n",
    "    samples_tested=0\n",
    "    correct=0\n",
    "    correct_per_class = {}\n",
    "    total_per_class = {}\n",
    "    with no_grad():\n",
    "        for batch_number,(x_test,y_test) in enumerate(dataloader):\n",
    "            x_test,y_test=x_test.to(device),y_test.to(device)\n",
    "            y_pred=model(x_test)\n",
    "\n",
    "            samples_tested+=y_test.size(0)\n",
    "            prediction=torch.max(y_pred,1)[1]\n",
    "            correct+=(prediction==y_test).sum()\n",
    "\n",
    "            for label in range(y_test.size(0)):\n",
    "                class_label = y_test[label].item()\n",
    "                if class_label not in correct_per_class:\n",
    "                    correct_per_class[class_label] = 0\n",
    "                    total_per_class[class_label] = 0\n",
    "                \n",
    "                total_per_class[class_label] += 1\n",
    "                if prediction[label] == y_test[label]:\n",
    "                    correct_per_class[class_label] += 1\n",
    "            #if (batch_number%100==0):\n",
    "            #    print(f\"Batch number: {batch_number}, Accuraccy: {correct/samples_tested}\")\n",
    "            #if (limit_batches==batch_number):\n",
    "            #    break\n",
    "    accuracy= correct/samples_tested\n",
    "    for class_label in sorted(total_per_class.keys()):\n",
    "        class_accuracy = correct_per_class[class_label] / total_per_class[class_label]\n",
    "        print(f\"Accuracy for class {class_label}: {class_accuracy:.4f}\")\n",
    "\n",
    "    return (accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spGSgN8PIpnL",
    "outputId": "f763613e-2738-47ec-b954-b8a7e8bbc539"
   },
   "outputs": [],
   "source": [
    "# Define model and optimizer\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=0)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "def init(path=\"\",CONTINUE_FROM_FILE=False,model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=0),optimizer=optim.Adam(model.parameters(),lr=0.001),continue_from=0,epoches=100):\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint_data={\"time\": 0, \"best_epoch\": (-1,99999999) }\n",
    "# Continue from previous model, if CONTINUE_FROM_FILE is set to True\n",
    "    if (CONTINUE_FROM_FILE):\n",
    "        checkpoint = load(path+'/checkpoint.pt', map_location=torch.device(device))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        checkpoint_data[\"time\"]=checkpoint[\"time\"]\n",
    "        checkpoint_data[\"best_epoch\"]=checkpoint[\"best_epoch\"]\n",
    "        continue_from=checkpoint[\"epoch\"]+1\n",
    "    print(checkpoint_data)\n",
    "    train_model(model,dataloader_train, checkpoint_data=checkpoint_data, optimizer=optimizer, continue_from_epoch=continue_from, epoches=epoches, dataloader_test=dataloader_test,limit_batches=-1,path=path)\n",
    "    with open(path+'/model.txt', 'w') as f:\n",
    "        print(model, file=f)\n",
    "        checkpoint = load(path+'/checkpoint2.pt', map_location=torch.device(device))\n",
    "        print(\"\\n\\n\",checkpoint, file=f)\n",
    "\n",
    "        \n",
    "#print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDKPbnINJBgH",
    "outputId": "618861ca-e0aa-498a-9755-42860a8bbaa7"
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "start = time.time()\n",
    "best_epoch=train_model(model,dataloader_train, checkpoint_data=checkpoint_data, optimizer=optimizer, continue_from_epoch=continue_from, epoches=epoches, dataloader_test=dataloader_test,limit_batches=-1)\n",
    "stop=time.time()\n",
    "print(f\"Total training time: {stop-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWEDgBYWFNKZ"
   },
   "outputs": [],
   "source": [
    "# Test Model\n",
    "accuracy_test=test_model(model,dataloader_test,limit_batches=-1)\n",
    "accuracy_train=test_model(model,dataloader_train,limit_batches=-1)\n",
    "print(f\"Accuracy on Test: {accuracy_test}, Accuracy on Train: {accuracy_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "m3qS5TQFe3-A",
    "outputId": "0c7d35d5-be32-44b7-8e8d-1465d7d3f2f5"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean and std of the dataset for normalization\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "for data, _ in dataloader_train:\n",
    "    batch_samples = data.size(0)\n",
    "    data = data.view(batch_samples, data.size(1), -1)\n",
    "    mean += data.mean(2).sum(0)\n",
    "    std += data.std(2).sum(0)\n",
    "    nb_samples += batch_samples\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "#file=open(\"/Downloads/mean_std.txt\",\"w\")\n",
    "print(\"Mean: \"+str(mean)+\"\\n\"+\"Std: \"+str(std))\n",
    "#file.write(\"Mean: \"+str(mean)+\"\\n\"+\"Std: \"+str(std))\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.txt', 'w') as f:\n",
    "    print(model, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=100\n",
    "dropout_percent=0.2\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=100,conv2_output_layers=70)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "init(path=\"/home/i/ioakeime/results/adam/lr_0.001/conv1_output_layers_100_conv2_output_layers_70\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=36,conv2_output_layers=50)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "init(path=\"/home/i/ioakeime/results/adam/lr_0.001/conv1_output_layers_36_conv2_output_layers_50\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=100,conv2_output_layers=70)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
    "init(path=\"/home/i/ioakeime/results/adam/lr_0.0001/conv1_output_layers_100_conv2_output_layers_70\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=36,conv2_output_layers=50)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
    "init(path=\"/home/i/ioakeime/results/adam/lr_0.0001/conv1_output_layers_36_conv2_output_layers_50\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=100,conv2_output_layers=70)\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.001)\n",
    "init(path=\"/home/i/ioakeime/results/sgd/lr_0.001/conv1_output_layers_100_conv2_output_layers_70\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=36,conv2_output_layers=50)\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.001)\n",
    "init(path=\"/home/i/ioakeime/results/sgd/lr_0.001/conv1_output_layers_36_conv2_output_layers_50\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=100,conv2_output_layers=70)\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.0001)\n",
    "init(path=\"/home/i/ioakeime/results/sgd/lr_0.0001/conv1_output_layers_100_conv2_output_layers_70\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n",
    "\n",
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=dropout_percent,conv1_output_layers=36,conv2_output_layers=50)\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.0001)\n",
    "init(path=\"/home/i/ioakeime/results/sgd/lr_0.0001/conv1_output_layers_36_conv2_output_layers_50\",CONTINUE_FROM_FILE=False,model=model,optimizer=optimizer,continue_from=0,epoches=max_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralNetwork(output_features=len(data_train.classes),dropout_percent=0.2,conv1_output_layers=100,conv2_output_layers=70)\n",
    "model.to(device)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
    "checkpoint = load('/home/i/ioakeime/results/adam/lr_0.0001/conv1_output_layers_100_conv2_output_layers_70/checkpoint2.pt', map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "test_model_and_classes(model,dataloader_train,limit_batches=-1)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
