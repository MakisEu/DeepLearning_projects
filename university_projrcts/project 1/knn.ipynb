{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lRbD3t1aDqdg"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch import nn,cuda,backends, optim, save, load, no_grad\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder,CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E70DP-AdEyZx"
   },
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "mean=[0.4914, 0.4822, 0.4465]\n",
    "std=[0.2023, 0.1994, 0.2010]\n",
    "\n",
    "#mean=[0.0178, 0.0574, 0.2181]\n",
    "#std=[1.0364, 1.0332, 1.0443]\n",
    "\n",
    "transformations=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomInvert(p=0.2),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=mean,std=std),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77hzGrZjFpEB",
    "outputId": "da47c8d8-29d3-4ba7-acaf-3d8e3b1bd9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size=50\n",
    "# select classes you want to include in your subset\n",
    "classes = torch.tensor([3, 8])\n",
    "classes_names= [\"cat\",\"ship\"]\n",
    "\n",
    "data_train = CIFAR10(root='./data', train=True, download=True, transform=transformations)\n",
    "indices = (torch.tensor(data_train.targets)[..., None] == classes).any(-1).nonzero(as_tuple=True)[0]\n",
    "data_train = torch.utils.data.Subset(data_train, indices)\n",
    "#classes_names=data_train.classes\n",
    "dataloader_train = torch.utils.data.DataLoader(data_train,batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "data_test = CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "indices = (torch.tensor(data_test.targets)[..., None] == classes).any(-1).nonzero(as_tuple=True)[0]\n",
    "data_test = torch.utils.data.Subset(data_test, indices)\n",
    "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8OheLDGF5ZW",
    "outputId": "e36a96c1-2cb6-47e8-abbe-699d827f7de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get device for pytorch to run on\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_1=KNeighborsClassifier(n_neighbors=1)\n",
    "knn_3=KNeighborsClassifier(n_neighbors=3)\n",
    "nc=NearestCentroid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, (3072,))\n"
     ]
    }
   ],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "for x_batch,y_batch in dataloader_train:\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        x_train.append(x_batch[i].detach().numpy().flatten())\n",
    "        y_train.append(y_batch[i].detach().numpy().flatten())\n",
    "    \n",
    "print((len(x_train),x_train[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, (3072,))\n"
     ]
    }
   ],
   "source": [
    "x_test=[]\n",
    "y_test=[]\n",
    "for x_batch,y_batch in dataloader_test:\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        x_test.append(x_batch[i].detach().numpy().flatten())\n",
    "        y_test.append(y_batch[i].detach().numpy().flatten())\n",
    "    \n",
    "print((len(x_test),x_test[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training_knn_1=time.time()\n",
    "knn_1.fit(x_train,np.ravel(y_train))\n",
    "stop_training_knn_1=time.time()\n",
    "\n",
    "start_testing_knn_1=time.time()\n",
    "acc_knn_1=knn_1.score(x_test, y_test)\n",
    "stop_testing_knn_1=time.time()\n",
    "\n",
    "p = pickle.dumps(knn_1)\n",
    "memory_knn_1=sys.getsizeof(p)\n",
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training_knn_3=time.time()\n",
    "knn_3.fit(x_train,np.ravel(y_train))\n",
    "stop_training_knn_3=time.time()\n",
    "\n",
    "start_testing_knn_3=time.time()\n",
    "acc_knn_3=knn_3.score(x_test, y_test)\n",
    "stop_testing_knn_3=time.time()\n",
    "\n",
    "p = pickle.dumps(knn_3)\n",
    "memory_knn_3=sys.getsizeof(p)\n",
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training_nc=time.time()\n",
    "nc.fit(x_train,np.ravel(y_train))\n",
    "stop_training_nc=time.time()\n",
    "\n",
    "start_testing_nc=time.time()\n",
    "acc_nc=nc.score(x_test, y_test)\n",
    "stop_testing_nc=time.time()\n",
    "\n",
    "p = pickle.dumps(nc)\n",
    "memory_nc=sys.getsizeof(p)\n",
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Training time | Testing time | Accuracy | Memory used while idle (Bytes)\n",
      "KNN(K=1): 0.15329933166503906 | 1.061927080154419 | 0.747 | 122960721\n",
      "KNN(K=3): 0.05618762969970703 | 0.706660270690918 | 0.761 | 122960721\n",
      "NC: 0.10158920288085938 | 0.029914379119873047 | 0.6615 | 49597\n"
     ]
    }
   ],
   "source": [
    "# Print stats\n",
    "print(f\"Classifier: Training time | Testing time | Accuracy | Memory used while idle (Bytes)\")\n",
    "print(f\"KNN(K=1): {stop_training_knn_1-start_training_knn_1} | {stop_testing_knn_1-start_testing_knn_1} | {acc_knn_1} | {memory_knn_1}\")\n",
    "print(f\"KNN(K=3): {stop_training_knn_3-start_training_knn_3} | {stop_testing_knn_3-start_testing_knn_3} | {acc_knn_3} | {memory_knn_3}\")\n",
    "print(f\"NC: {stop_training_nc-start_training_nc} | {stop_testing_nc-start_testing_nc} | {acc_nc} | {memory_nc}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
